{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRIAN, THE STITCH, AND THE WARDROBE\n",
    "Imagine that on the floor in front of you is a pile of all\n",
    "the clothing you own—trousers, tops, shoes, and coats,\n",
    "all of different styles. Your stylist, Brian, is becoming\n",
    "increasingly frustrated with how long it takes him to\n",
    "find the items you require, so he devises a clever plan.\n",
    "He tells you to organize your clothes into a wardrobe\n",
    "that is infinitely high and wide (Figure 3-1). When you\n",
    "want to request a particular item, you simply need to\n",
    "tell Brian its location and he will sew the item from\n",
    "scratch using his trusty sewing machine. It soon\n",
    "becomes obvious that you will need to place similar\n",
    "items near to each other, so that Brian can accurately\n",
    "re-create each item given only its location.\n",
    "\n",
    "After several weeks of practice, you and Brian have\n",
    "adjusted to each other’s understandings of the wardrobe\n",
    "layout. It is now possible for you to tell Brian the location of any item of clothing that you desire, and he\n",
    "can accurately sew it from scratch!\n",
    "This gives you an idea—what would happen if you gave\n",
    "Brian a wardrobe location that was empty? To your\n",
    "amazement, you find that Brian is able to generate\n",
    "entirely new items of clothing that haven’t existed\n",
    "before! The process isn’t perfect, but you now have\n",
    "limitless options for generating new clothing, just by\n",
    "picking an empty location in the infinite wardrobe and\n",
    "letting Brian work his magic with the sewing machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 5s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "(x_train,y_train), (x_test,y_test) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(imgs):\n",
    "    imgs = imgs.astype('float32')/255.0\n",
    "    imgs = np.pad(imgs, ((0,0),(2,2),(2,2)), constant_values=0.0)\n",
    "    imgs = np.expand_dims(imgs,-1)\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_train = preprocess(x_train)\n",
    "x_test = preprocess(x_test)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is a neural network made up of two parts:\n",
    "- An encoder network that compresses high-dimensional\n",
    "input data such as an image into a lower-dimensional\n",
    "embedding vector\n",
    "- A decoder network that decompresses a given\n",
    "embedding vector back to the original domain (e.g.,\n",
    "back to an image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,models\n",
    "import keras.backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_47 (Conv2D)          (None, 16, 16, 32)        320       \n",
      "                                                                 \n",
      " conv2d_48 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_49 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 2048)              0         \n",
      "                                                                 \n",
      " encoder_output (Dense)      (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96770 (378.01 KB)\n",
      "Trainable params: 96770 (378.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_input = layers.Input(shape=(32,32,1),name='encoder_input')\n",
    "x = layers.Conv2D(filters=32\n",
    "                  ,kernel_size=3\n",
    "                  ,strides=2\n",
    "                  ,activation='relu'\n",
    "                  ,padding='same')(encoder_input)\n",
    "x = layers.Conv2D(filters=64\n",
    "                  ,kernel_size=3\n",
    "                  ,strides=2\n",
    "                  ,activation='relu'\n",
    "                  ,padding='same')(x)\n",
    "x = layers.Conv2D(filters=128\n",
    "                  ,kernel_size=3\n",
    "                  ,strides=2\n",
    "                  ,activation='relu'\n",
    "                  ,padding='same')(x)\n",
    "\n",
    "shape_before_flattening = K.int_shape(x)[1:]\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "encoder_output = layers.Dense(units=2,name='encoder_output')(x)\n",
    "model = models.Model(encoder_input,encoder_output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The convolutional transpose layer uses the same\n",
    "principle as a standard convolutional layer (passing a\n",
    "filter across the image), but is different in that setting\n",
    "strides = 2 doubles the size of the input tensor in both\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(shape=(2,), name='decoder_input')\n",
    "x = layers.Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "x = layers.Reshape(shape_before_flattening)(x)\n",
    "x = layers.Conv2DTranspose(\n",
    "    128,(3,3)\n",
    "    ,strides=2\n",
    "    ,activation='relu'\n",
    "    ,padding='same'\n",
    ")(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    64,(3,3)\n",
    "    ,strides=2\n",
    "    ,activation='relu'\n",
    "    ,padding='same'\n",
    ")(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(\n",
    "    32,(3,3)\n",
    "    ,strides=2\n",
    "    ,activation='relu'\n",
    "    ,padding='same'\n",
    ")(x)\n",
    "\n",
    "decoder_output = layers.Conv2D(\n",
    "    1,(3,3)\n",
    "    ,strides=1\n",
    "    ,activation='sigmoid'\n",
    "    ,padding='same'\n",
    "    ,name='decoder_output'\n",
    ")(x)\n",
    "decoder = models.Model(decoder_input, decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = models.Model(encoder_input, decoder(encoder_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
